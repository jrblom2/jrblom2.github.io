---
layout: project
permalink: /:title/
category: projects

meta:
  keywords: "Human Interaction, Control"

project:
  title: "Learning through Shared Control"
  type: "Demonstration Learning"
  url: "https://github.com/jrblom2/Shared-Learning-with-Admittance-Control"
  logo: "/assets/images/projects/ergodic/cover.gif"
  tech: "Demonstration Learning, Shared Control, Python, C++, Optimal Control"
  year: "2025"
  order: 2

autovideos:
  - video:
    url: "/assets/videos/projects/ergodic/triangle.mp4"

videos:
  - video:
    url: "/assets/videos/projects/ergodic/square.mp4"

images:
  - image:
    url: "/assets/images/projects/ergodic/pose3.jpg"
    alt: "Mechanical setup"

---
<span class="h2">Project</span>
<p> This project develops a system for demonstration learning using ergodic metrics and optimal control to teach a robot to draw simple shapes like triangles and squares through user interaction. A human demonstrates both correct and incorrect ways to perform a task based on whether the pen is up or down, denoted with blue and red respectively. The robot learns from these demonstrations and refines its understanding of the task over time, represented by an evolving coefficient map. It then guides the user through what it has learned, allowing for continuous refinement through shared control.
</p>

<span class="h2">Integrated System</span>
<img src="/assets/images/projects/ergodic/ergodicSystem.svg" alt="System Diagram" style="width: 60%;">

<span class="h2">Method</span>
<p> The system begins with either a blank map (for broad exploration) or a rough demonstration of the task from a user demo. This initial input is used to model the task as a spatial distribution using a Fourier transform, with cosine basis functions. This distribution forms the basis for an ergodic metric, a measure of how well a trajectory covers a target distribution. This metric is embedded in a cost function, which drives trajectory optimization using iLQR (iterative Linear Quadratic Regulator) in the planning stage.</p>

<img src="/assets/images/projects/ergodic/ergodicStates.svg" alt="System Diagram" style="width: 80%;">
<p>
The resulting trajectory tends to revisit important regions from the original demonstration while guiding the user. As the robot executes this trajectory, the human collaborates in real timeâ€”making small corrections and deciding when to take specific actions (like lifting or lowering the pen). These interactions feed back into the system, updating the coefficient map to reflect the user's input when the trajectory is finished and the planning is redone.
</p>
<span class="h2"></span>
<img src="/assets/images/projects/ergodic/update.gif" alt="Map update demo" style="width: 50%;">
<span class="h2"></span>
<p>
Over time, the system refines the task definition by integrating both positive and negative demonstrations, enabling a more nuanced understanding and improved execution.
</p>
